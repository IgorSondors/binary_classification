{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pth_models = \"/home/sondors/Documents/price/BERT_data/data/10-04-2024_Timofey/2801_models_Apple.csv\"\n",
    "pth_models = \"/home/sondors/Documents/price/BERT_data/data/17-04-2024_Timofey/2801_offers_models_Apple.csv\"\n",
    "df_models = pd.read_csv(pth_models, sep=';')\n",
    "\n",
    "pth_offers = '/home/sondors/Documents/price/BERT_data/data/17-04-2024_Timofey/2801_Apple_triplets_offer_model_train.csv'\n",
    "df_offers = pd.read_csv(pth_offers, sep=';')\n",
    "\n",
    "pth_offers_test = '/home/sondors/Documents/price/BERT_data/data/17-04-2024_Timofey/2801_offers_test_Apple.csv'\n",
    "df_offers_test = pd.read_csv(pth_offers, sep=';')\n",
    "\n",
    "id_category = {\n",
    "    # 3902: 'диктофоны, портативные рекордеры',\n",
    "    # 510402: 'электронные книги',\n",
    "    # 4302: 'автомобильные телевизоры, мониторы',\n",
    "    # 2815: 'смарт-часы и браслеты',\n",
    "    # 3901: 'портативные медиаплееры',\n",
    "    # 3904: 'портативная акустика',\n",
    "    2801: 'мобильные телефоны',\n",
    "    # 3908: 'VR-гарнитуры (VR-очки, шлемы, очки виртуальной реальности, FPV очки для квадрокоптеров)',\n",
    "    # 510401: 'планшетные компьютеры и мини-планшеты',\n",
    "    # 2102: 'наушники, гарнитуры, наушники c микрофоном',\n",
    "    # 3903: 'радиоприемники, радиобудильники, радиочасы',\n",
    "    # 3907: 'магнитолы',\n",
    "    # 280801: 'GPS-навигаторы'\n",
    "    }\n",
    "\n",
    "df_models = df_models[df_models['category_id'].isin(id_category.keys())].reset_index(drop=True)\n",
    "df_offers = df_offers[df_offers['category_id'].isin(id_category.keys())].reset_index(drop=True)\n",
    "df_offers = df_offers#[:10000]\n",
    "\n",
    "df_offers_shuffled = df_offers.sample(frac=1, random_state=42)\n",
    "\n",
    "# Определяем размер тестовой выборки (например, 20%)\n",
    "test_size = int(0.15 * len(df_offers_shuffled))\n",
    "\n",
    "# Разделяем данные на тренировочную и тестовую выборки\n",
    "df_train = df_offers_shuffled.iloc[:-test_size]\n",
    "df_test = df_offers_shuffled.iloc[-test_size:]\n",
    "\n",
    "# Пример: вывод размеров тренировочной и тестовой выборок\n",
    "print(\"Размер тренировочной выборки:\", len(df_train))\n",
    "print(\"Размер тестовой выборки:\", len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interface import get_query_emb_batch, load_model, cosine_similarity_batch\n",
    "from typing import Tuple, List, Dict, Union, Any\n",
    "from colbert.modeling.checkpoint import Checkpoint\n",
    "\n",
    "# ckpt_pth = \"/home/sondors/Documents/ColBERT_weights/triples_X1_13_categories_use_ib_negatives/none/2024-01/26/10.49.44/checkpoints/colbert-5387-finish\"\n",
    "# experiment = \"colbert-5387\"\n",
    "\n",
    "ckpt_pth = \"/home/sondors/Documents/ColBERT_weights/2801_lr04_bsize_210_apple/none/2024-04/18/09.16.10/checkpoints/colbert-187-finish\"\n",
    "experiment = \"colbert-187-finish\"\n",
    "\n",
    "doc_maxlen = 300\n",
    "nbits = 2   # bits определяет количество битов у каждого измерения в семантическом пространстве во время индексации\n",
    "nranks = 1  # nranks определяет количество GPU для использования, если они доступны\n",
    "kmeans_niters = 4 # kmeans_niters указывает количество итераций k-means кластеризации; 4 — хороший и быстрый вариант по умолчанию. \n",
    "\n",
    "device = \"cuda\"\n",
    "checkpoint = load_model(ckpt_pth, doc_maxlen, nbits, kmeans_niters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "def get_query_emb(sentences: List[str], checkpoint: Checkpoint, batch_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of sentences using the provided checkpoint.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): A list of sentences for which embeddings need to be generated.\n",
    "        checkpoint (Checkpoint): The checkpoint object used for generating embeddings.\n",
    "        batch_size (int): The batch size to use during inference.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of embeddings for the input sentences.\n",
    "    \"\"\"\n",
    "    return checkpoint.queryFromText(sentences, bsize=batch_size).to(\"cpu\").numpy()\n",
    "\n",
    "def get_query_emb_batch(sentences: List[str], checkpoint: Checkpoint, batch_size: int, batch_size2: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of sentences in batches using the provided checkpoint.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): A list of sentences for which embeddings need to be generated.\n",
    "        checkpoint (Checkpoint): The checkpoint object used for generating embeddings.\n",
    "        batch_size (int): The batch size to use during inference.\n",
    "        batch_size2 (int): The size of the sub-batches to split the input sentences into.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of embeddings for the input sentences. \n",
    "        Shape of the array is (len(sentences), 32, 768) for bert-base-multilingual-cased or (len(sentences), 32, 128) for colbertv2.0\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(sentences), batch_size2):\n",
    "        # print(f\"batch: {min(i+batch_size2, len(sentences))}/{len(sentences)}\")\n",
    "\n",
    "        batch_sentences = sentences[i:i+batch_size2]\n",
    "        embeddings = get_query_emb(batch_sentences, checkpoint, batch_size)\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    combined_embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    return combined_embeddings\n",
    "\n",
    "class MyDataset_old(Dataset):\n",
    "    def __init__(self, offer_batch, true_match_batch, false_match_batch, checkpoint):\n",
    "        self.offer_batch = offer_batch\n",
    "        self.true_match_batch = true_match_batch\n",
    "        self.false_match_batch = false_match_batch\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.offer_batch)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        offer_embs = get_query_emb_batch([self.offer_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "        true_match_embs = get_query_emb_batch([self.true_match_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "        false_match_embs = get_query_emb_batch([self.false_match_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "\n",
    "        y_true = np.ones(len(true_match_embs))  # Метка 1 для пар (offer_embs[i], true_match_embs[i])\n",
    "        y_false = np.zeros(len(false_match_embs))  # Метка 0 для пар (offer_embs[i], false_match_embs[i])\n",
    "\n",
    "        X_true = np.stack([offer_embs, true_match_embs], axis=1)\n",
    "        X_false = np.stack([offer_embs, false_match_embs], axis=1)\n",
    "\n",
    "        X = np.concatenate([X_true, X_false])\n",
    "        y = np.concatenate([y_true, y_false])\n",
    "\n",
    "        X_tensor = torch.tensor(X.reshape(X.shape[0], -1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)  # изменено на float32\n",
    "\n",
    "        # print(f\"offer_embs: {np.shape(offer_embs)}\")\n",
    "        # print(f\"true_match_embs: {np.shape(true_match_embs)}\")\n",
    "        # print(f\"false_match_embs: {np.shape(false_match_embs)}\")\n",
    "        # print(f\"y_true: {np.shape(y_true)}\")\n",
    "        # print(f\"y_false: {np.shape(y_false)}\")\n",
    "        # print(f\"X_true: {np.shape(X_true)}\")\n",
    "        # print(f\"X_false: {np.shape(X_false)}\")\n",
    "        # print(f\"X: {np.shape(X)}\")\n",
    "        # print(f\"y: {np.shape(y)}\")\n",
    "        # print(f\"X_tensor: {np.shape(X_tensor)}\")\n",
    "        # print(f\"y_tensor: {np.shape(y_tensor)}\")\n",
    "\n",
    "        return X_tensor, y_tensor\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, offer_batch, true_match_batch, false_match_batch, checkpoint):\n",
    "        self.offer_batch = offer_batch\n",
    "        self.true_match_batch = true_match_batch\n",
    "        self.false_match_batch = false_match_batch\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.offer_batch)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        offer_embs = get_query_emb_batch([self.offer_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "        true_match_embs = get_query_emb_batch([self.true_match_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "        false_match_embs = get_query_emb_batch([self.false_match_batch[idx]], self.checkpoint, batch_size=100, batch_size2=1000)\n",
    "\n",
    "        y_true = np.ones(len(true_match_embs))  # Метка 1 для пар (offer_embs[i], true_match_embs[i])\n",
    "        y_false = np.zeros(len(false_match_embs))  # Метка 0 для пар (offer_embs[i], false_match_embs[i])\n",
    "\n",
    "        X = np.concatenate([offer_embs, offer_embs])#.reshape(-1,32,768)\n",
    "        X_pair = np.concatenate([true_match_embs, false_match_embs])#.reshape(-1,32,768)\n",
    "        y = np.concatenate([y_true, y_false])#.reshape(-1)\n",
    "\n",
    "        # X_tensor = torch.tensor(X.reshape(X.shape[0], -1), dtype=torch.float32)\n",
    "        # X_pair_tensor = torch.tensor(X_pair.reshape(X_pair.shape[0], -1), dtype=torch.float32)\n",
    "        # y_tensor = torch.tensor(y, dtype=torch.float32)  # изменено на float32\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        X_pair_tensor = torch.tensor(X_pair, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # print(f\"offer_embs: {np.shape(offer_embs)}\")\n",
    "        # print(f\"true_match_embs: {np.shape(true_match_embs)}\")\n",
    "        # print(f\"false_match_embs: {np.shape(false_match_embs)}\")\n",
    "        # print(f\"y_true: {np.shape(y_true)}\")\n",
    "        # print(f\"y_false: {np.shape(y_false)}\")\n",
    "        # print(f\"X_true: {np.shape(X_true)}\")\n",
    "        # print(f\"X_false: {np.shape(X_false)}\")\n",
    "        # print(f\"X: {np.shape(X)}\")\n",
    "        # print(f\"y: {np.shape(y)}\")\n",
    "        # print(f\"X_tensor: {np.shape(X_tensor)}\")\n",
    "        # print(f\"y_tensor: {np.shape(y_tensor)}\")\n",
    "\n",
    "        return X_tensor, X_pair_tensor, y_tensor\n",
    "    \n",
    "# Используем созданный класс для создания загрузчика данных\n",
    "df = df_train.copy()\n",
    "offer_batch = list(df['name'])\n",
    "true_match_batch = list(df['true_match'])\n",
    "false_match_batch = list(df['false_match'])\n",
    "train_dataset = MyDataset(offer_batch, true_match_batch, false_match_batch, checkpoint)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "df = df_test.copy()\n",
    "offer_batch = list(df['name'])\n",
    "true_match_batch = list(df['true_match'])\n",
    "false_match_batch = list(df['false_match'])\n",
    "test_dataset = MyDataset(offer_batch, true_match_batch, false_match_batch, checkpoint)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# for X_tensor, X_pair_tensor, y_tensor in train_dataloader:\n",
    "#     print(f\"X_tensor: {np.shape(X_tensor)}\")\n",
    "#     print(f\"X_pair_tensor: {np.shape(X_pair_tensor)}\")\n",
    "#     print(f\"y_tensor: {np.shape(y_tensor)}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=(32, 768), latent_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim[0] * input_dim[1], 1024)  # Flatten input\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc31 = nn.Linear(256, latent_dim)\n",
    "        self.fc32 = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the matrix to a vector\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, output_dim=(32, 768)):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1024)\n",
    "        self.fc3 = nn.Linear(1024, output_dim[0] * output_dim[1])\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h)).view(-1, self.output_dim[0], self.output_dim[1])  # Reshape to original\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        h1 = F.relu(self.fc1(z1))\n",
    "        h2 = F.relu(self.fc1(z2))\n",
    "        diff = torch.abs(h1 - h2)\n",
    "        out = F.relu(self.fc2(diff))\n",
    "        return torch.sigmoid(self.fc3(out)).squeeze(1)  # Ensure this line produces a shape of [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def validate(vae, siamese, dataloader):\n",
    "    vae.eval()\n",
    "    siamese.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    mean_vae_time = 0\n",
    "    mean_seamese_time = 0\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for f, g, similarity in dataloader:\n",
    "            # Process through VAE\n",
    "            f, g, similarity = f.reshape(-1,32,768), g.reshape(-1,32,768), similarity.reshape(-1)\n",
    "\n",
    "            recon_f, mu_f, log_var_f = vae(f.view(f.size(0), -1))\n",
    "            recon_g, mu_g, log_var_g = vae(g.view(g.size(0), -1))\n",
    "\n",
    "            vae_time_start = time.time()\n",
    "            loss_vae_f = vae_loss(recon_f, f, mu_f, log_var_f)\n",
    "            loss_vae_g = vae_loss(recon_g, g, mu_g, log_var_g)\n",
    "            mean_vae_time += time.time() - vae_time_start\n",
    "\n",
    "            siamese_time_start = time.time()\n",
    "            similarity_score = siamese(vae.reparameterize(mu_f, log_var_f), vae.reparameterize(mu_g, log_var_g))\n",
    "            mean_seamese_time += time.time() - siamese_time_start\n",
    "\n",
    "            loss_siamese = F.binary_cross_entropy(similarity_score, similarity.view(-1))\n",
    "\n",
    "            loss = loss_vae_f + loss_vae_g + loss_siamese\n",
    "\n",
    "            # Update total validation loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted_labels = (similarity_score > 0.5).float()\n",
    "            correct_predictions += (predicted_labels == similarity.view(-1)).sum().item()\n",
    "            total_samples += similarity.size(0)\n",
    "\n",
    "            # Collect all predictions and actual labels for ROC AUC calculation\n",
    "            all_predictions.extend(similarity_score.detach().cpu().numpy())\n",
    "            all_targets.extend(similarity.cpu().numpy())\n",
    "\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    roc_auc = roc_auc_score(all_targets, all_predictions)  # Compute ROC AUC\n",
    "    mean_vae_time = mean_vae_time / total_samples\n",
    "    mean_seamese_time = mean_seamese_time / total_samples\n",
    "\n",
    "    return average_loss, accuracy, roc_auc, mean_vae_time, mean_seamese_time\n",
    "\n",
    "def train(vae, siamese, train_dataloader, val_dataloader, optimizer, epochs=15):\n",
    "    for epoch in range(epochs):\n",
    "        vae.train()\n",
    "        siamese.train()\n",
    "        total_train_loss = 0\n",
    "        for f, g, similarity in train_dataloader:\n",
    "            f, g, similarity = f.reshape(-1,32,768), g.reshape(-1,32,768), similarity.reshape(-1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_f, mu_f, log_var_f = vae(f)\n",
    "            recon_g, mu_g, log_var_g = vae(g)\n",
    "\n",
    "            loss_vae_f = vae_loss(recon_f, f, mu_f, log_var_f)\n",
    "            loss_vae_g = vae_loss(recon_g, g, mu_g, log_var_g)\n",
    "\n",
    "            similarity_score = siamese(vae.reparameterize(mu_f, log_var_f), vae.reparameterize(mu_g, log_var_g))\n",
    "            loss_siamese = F.binary_cross_entropy(similarity_score, similarity.view(-1))\n",
    "\n",
    "            total_loss = loss_vae_f + loss_vae_g + loss_siamese\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += total_loss.item()\n",
    "\n",
    "        average_train_loss = total_train_loss / len(train_dataloader)\n",
    "        average_val_loss, accuracy, roc_auc, mean_vae_time, mean_seamese_time = validate(vae, siamese, val_dataloader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {average_train_loss}, Validation Loss: {average_val_loss}, Val ROCAUC: {roc_auc}, Time VAE|Seamese: {mean_vae_time}|{mean_seamese_time}')\n",
    "    return vae, siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VAE()\n",
    "model_vae.load_state_dict(torch.load('vae_epoch_20.pth'))\n",
    "model_siamese = SiameseNetwork()\n",
    "model_siamese.load_state_dict(torch.load('siamese_epoch_20.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test.copy()\n",
    "offer_batch = list(df['name'])\n",
    "true_match_batch = list(df['true_match'])\n",
    "false_match_batch = list(df['false_match'])\n",
    "test_dataset = MyDataset(offer_batch, true_match_batch, false_match_batch, checkpoint)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_val_loss, accuracy, roc_auc, mean_vae_time, mean_seamese_time = validate(model_vae, model_siamese, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average_val_loss = {average_val_loss}, accuracy = {accuracy}, roc_auc = {roc_auc}, mean_vae_time = {mean_vae_time}, mean_seamese_time = {mean_seamese_time}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
